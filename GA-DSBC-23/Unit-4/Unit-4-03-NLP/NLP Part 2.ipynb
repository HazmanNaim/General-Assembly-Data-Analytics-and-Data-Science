{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b44b276",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# NLP Part 2: `CountVectorizer` and `TfidfVectorizer`\n",
    "\n",
    "\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "- Extract features from unstructured text by fitting and transforming with `CountVectorizer` and `TfidfVectorizer`.\n",
    "- Describe how CountVectorizers and TF-IDFVectorizers work.\n",
    "- Understand `stop_words`, `max_features`, `min_df`, `max_df`, and `ngram_range`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ef5e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import CountVectorizer and TFIDFVectorizer from feature_extraction.text.\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe47428",
   "metadata": {},
   "source": [
    "# Introduction to Text Feature Extraction\n",
    "\n",
    "The models we've learned, like linear regression, logistic regression, and k-nearest neighbors, take in an `X` and a `y` variable.\n",
    "- `X` is a matrix/dataframe of independent variables.\n",
    "- `y` is a vector/series of representing the target variable.\n",
    "\n",
    "Text data (also called natural language data) is not already organized as a matrix or vector of real numbers. We say that this data is **unstructured**.\n",
    "\n",
    "> This lesson will focus on how to transform our unstructured text data into a numeric `X` matrix. This matrix is known as a term-document matrix, where each document is a row of the matrix and each column represents a frequency count of the occurence of the term in each document.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9520d84",
   "metadata": {},
   "source": [
    "## Basic terminology\n",
    "\n",
    "---\n",
    "\n",
    "- A collection of text is a **document**. \n",
    "    - You can think of a document as a row in your feature matrix.\n",
    "- A collection of documents is a **corpus**. \n",
    "    - You can think of your full dataframe as the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58b791f",
   "metadata": {},
   "source": [
    "# Count Vectorizer\n",
    "\n",
    "In order to use unstructured data, first we have to put it in a structured format. \n",
    "\n",
    "Consider a list of job descriptions for data scientists extracted from LinkedIn. We can consider each job description as a document, and we want to find out how many common job requirements there are, in all the documents.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0597b225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First define the corpus of documents (body of documents)\n",
    "\n",
    "jobs_data = pd.read_csv('data/jobs.csv')\n",
    "jobs_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695b5114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a look at the full description for the first job\n",
    "jobs_data['description'][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71ae6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer requires a vector, so we need to get the series from the DataFrame. \n",
    "jobs = jobs_data['description']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7772889a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the CountVectorizer\n",
    "cvec = CountVectorizer(binary=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6020c378",
   "metadata": {},
   "source": [
    "Some important default hyperparameters to consider first:\n",
    "- `analyzer = 'word'` : the features to be identified are words\n",
    "- `token_pattern = r'(?u)\\b\\w\\w+\\b'` : tokens consist of two or more alphanumeric characters\n",
    "- `binary = False` : each occurence of the token is counted, if `True` then the matrix only records whether the token exists in the document.\n",
    "\n",
    "Let's see the results when we fit the CountVectorizer to the `jobs` corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80e8442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the count vectorizer to the corpus\n",
    "cvec.fit(jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b032033",
   "metadata": {},
   "source": [
    "When we have unstructured text data, there is a lot of information in that text data.\n",
    "- When we force unstructured text data to follow a \"spreadsheet\" or \"dataframe\" structure, we might lose some of that information.\n",
    "- For example, CountVectorizer creates a vector (column) for each token and counts up the number of occurrences of each token in each document.\n",
    "\n",
    "Our tokens are now stored as a **bag-of-words**. This is a simplified way of looking at and storing our data. \n",
    "- Bag-of-words representations discard grammar, order, and structure in the text but track occurrences.\n",
    "\n",
    "<img src=\"images/countvectorizer.png\" alt=\"drawing\" width=\"750\"/>\n",
    "\n",
    "[Source](https://towardsdatascience.com/nlp-learning-series-part-2-conventional-methods-for-text-classification-40f2839dd061).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a663d0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The count vectorizer identifies each token as a feature. \n",
    "cvec.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5ecefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform into a term-document matrix\n",
    "cv_matrix = cvec.transform(jobs)\n",
    "cv_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9654ebc8",
   "metadata": {},
   "source": [
    "## Term-Document Matrix\n",
    "\n",
    "As you can see, the matrix is in 'Compressed Sparse Row format', as there are many zeros. This is because many of the tokens appear only in one of the documents, but they are still assigned to one whole column of the matrix.\n",
    "\n",
    "Let's print the matrix to see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b398b79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cv_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437f776e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting to a dense matrix (show the whole matrix, including all the zeros)\n",
    "cv_dense = cv_matrix.todense()\n",
    "cv_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516294b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format as a dataframe to see the terms and documents\n",
    "df = pd.DataFrame(data=cv_dense,columns = cvec.get_feature_names())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b532665",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plot top occuring words\n",
    "df.sum().sort_values(ascending=False).head(20).plot(kind='barh');\n",
    "\n",
    "# Find the top 20 occurring instead. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47593b0",
   "metadata": {},
   "source": [
    "Creating a term-document matrix like this is also known as a bag-of-words approach.\n",
    "\n",
    "<details><summary>What might be some of the advantages of using this bag-of-words approach when modeling?</summary>\n",
    "\n",
    "- Efficient to store.\n",
    "- Efficient to model.\n",
    "- Keeps a decent amount of information.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1526efbb",
   "metadata": {},
   "source": [
    "<details><summary>What might be some of the disadvantages of using this bag-of-words approach when modeling?</summary>\n",
    "\n",
    "- Since bag-of-words models discard grammar, order, structure, and context, we lose a decent amount of information.\n",
    "- Phrases like \"not bad\" or \"not good\" won't be interpreted properly.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40610c55",
   "metadata": {},
   "source": [
    "Let's see if we can process from the corpus so that it gives us more meaningful information.\n",
    "\n",
    "We will consider some of the different hyperparameters of `CountVectorizer`:\n",
    "- `stop_words`\n",
    "- `max_features`, `max_df`, `min_df`\n",
    "- `ngram_range`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f7687f",
   "metadata": {},
   "source": [
    " ## Stopwords\n",
    " \n",
    " Notice that there are many common words ('and','to', 'the') in the term-document matrix, which may not be very useful for analyzing common job requirements. \n",
    "\n",
    "`CountVectorizer` gives you the option to eliminate stopwords from your corpus when instantiating your vectorizer.\n",
    "\n",
    "```python\n",
    "cvec = CountVectorizer(stop_words='english')\n",
    "```\n",
    "\n",
    "You can optionally pass your own list of stopwords that you'd like to remove.\n",
    "```python\n",
    "cvec = CountVectorizer(stop_words=['list', 'of', 'words', 'to', 'stop'])\n",
    "```\n",
    "or to add more stopwords to the default set:\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "xtra_stop_words = text.ENGLISH_STOP_WORDS.union(['list', 'of', 'words', 'to', 'stop'])\n",
    "cvec = CountVectorizer(stop_words=xtra_stop_words)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf75ef32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "\n",
    "xtra_stop_words = text.ENGLISH_STOP_WORDS.union(['data', 'science'])\n",
    "\n",
    "# instantiate the CountVectorizer with default english stopwords\n",
    "cvec_stopwords = CountVectorizer(stop_words=xtra_stop_words)\n",
    "\n",
    "# fit_transform is a more efficient way of performing both the fit and transform in one step\n",
    "cvec_stopwords.fit_transform(jobs)\n",
    "\n",
    "# how many features are identified?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99136086",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec_stopwords.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b44dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_matrix = cvec_stopwords.transform(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958e4b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4b57a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format as a dataframe to see the terms and documents\n",
    "df = pd.DataFrame(data=cv_matrix.todense(),columns = cvec_stopwords.get_feature_names())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561ba909",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plot top occuring words\n",
    "df.sum().sort_values(ascending=False).head(20).plot(kind='barh');\n",
    "\n",
    "# Find the top 20 occurring instead. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c78652",
   "metadata": {},
   "source": [
    "### Vocabulary size\n",
    "\n",
    "---\n",
    "One downside to `CountVectorizer` is the size of its vocabulary (`cvec.get_feature_names()`) can get really large. We're creating one column for every unique token in your corpus of data!\n",
    "\n",
    "There are three hyperparameters to help you control this.\n",
    "\n",
    "1. You can set `max_features` to only include the $N$ most popular vocabulary words in the corpus.\n",
    "\n",
    "```python\n",
    "cvec = CountVectorizer(max_features=1_000) # Only the top 1,000 words from the entire corpus will be saved\n",
    "```\n",
    "\n",
    "2. You can tell `CountVectorizer` to only consider words that occur in **at least** some number of documents (df = document frequency)\n",
    "\n",
    "```python\n",
    "cvec = CountVectorizer(min_df=2) # A word must occur in at least two documents from the corpus\n",
    "```\n",
    "\n",
    "3. Conversely, you can tell `CountVectorizer` to only consider words that occur in **at most** some percentage of documents.\n",
    "\n",
    "```python\n",
    "cvec = CountVectorizer(max_df=.98) # Ignore words that occur in > 98% of the documents from the corpus\n",
    "```\n",
    "\n",
    "Both `max_df` and `min_df` can accept either an integer or a float.\n",
    "- An integer tells us the number of documents.\n",
    "- A float tells us the percentage of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a37ca7",
   "metadata": {},
   "source": [
    "<details><summary>Why might we want to control these vocabulary size hyperparameters?</summary>\n",
    "    \n",
    "- If we have too many features, our models may take a **very** long time to fit.\n",
    "- Control for overfitting/underfitting.\n",
    "- Words in 99% of documents or words occuring in only one document might not be very informative.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed33f5a",
   "metadata": {},
   "source": [
    "### N-Gram Range\n",
    "---\n",
    "\n",
    "`CountVectorizer` has the ability to capture $n$-word phrases, also called $n$-grams. \n",
    "\n",
    "The `ngram_range` determines what $n$-grams should be considered as features.\n",
    "\n",
    "```python\n",
    "cvec = CountVectorizer(ngram_range=(2,2)) # Captures only 2-grams\n",
    "```\n",
    "\n",
    "```python\n",
    "cvec = CountVectorizer(ngram_range=(1,2)) # Captures every 1-gram and every 2-gram\n",
    "```\n",
    "Let's see the difference with our `jobs` documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a16559d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with different ngram_ranges and add to the list of stop words \n",
    "cvec = CountVectorizer(ngram_range=(2,2), stop_words='english', max_df=3, binary=True)\n",
    "\n",
    "cvec.fit(jobs)\n",
    "cv_matrix = cvec.transform(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e8d72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the feature names now?\n",
    "cvec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d053eae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store as a data frame\n",
    "df = pd.DataFrame(data=cv_matrix.todense(),columns = cvec.get_feature_names())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519158a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the top 20 occurring features\n",
    "# plot top occuring words\n",
    "df.sum().sort_values(ascending=False).head(20).plot(kind='barh');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8192502",
   "metadata": {},
   "source": [
    "Congratulations! We've used `CountVectorizer` to transform our text data into something we can pass into a model.\n",
    "\n",
    "But what if we want to do something more than just count up the occurrence of each token?\n",
    "\n",
    "## Term Frequency-Inverse Document Frequency (TF-IDF) Vectorizer\n",
    "\n",
    "---\n",
    "\n",
    "When modeling, which word do you think tends to be the most helpful?\n",
    "- Words that are common across all documents.\n",
    "- Words that are rare across all documents.\n",
    "- Words that are rare across some documents, and common across some documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922609c2",
   "metadata": {},
   "source": [
    "<details><summary>Answer:</summary>\n",
    "\n",
    "- Words that are common in certain documents but rare in other documents tend to be more informative than words that are common in all documents or rare in all documents.\n",
    "- Example: If we were examining poetry over time, the word \"thine\" might be common in some documents but rare in most documents. The word \"thine\" is probably pretty informative in this case.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8471a3",
   "metadata": {},
   "source": [
    "TF-IDF is a score that tells us which words are important to one document, relative to all other documents. Words that occur often in one document but don't occur in many documents contain more predictive power.\n",
    "\n",
    "Variations of the TF-IDF score are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query.\n",
    "- If you want to see how it can be calculated, check out [the Wikipedia page](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) and [`sklearn`](https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting) page.\n",
    "\n",
    "<img src=\"images/tfidfvectorizer.png\" alt=\"drawing\" width=\"750\"/>\n",
    "\n",
    "[Source](https://towardsdatascience.com/nlp-learning-series-part-2-conventional-methods-for-text-classification-40f2839dd061)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce80e7ac",
   "metadata": {},
   "source": [
    "### Practice Using the `TfidfVectorizer`\n",
    "\n",
    "---\n",
    "\n",
    "`sklearn` provides a TF-IDF vectorizer that works similarly to the CountVectorizer.\n",
    "- The arguments `stop_words`, `max_features`, `min_df`, `max_df`, and `ngram_range` also work here.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55612f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the transformer.\n",
    "tvec = TfidfVectorizer(ngram_range=(2,2), stop_words='english', max_df=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d137cc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and transform the job descriptions\n",
    "tv_matrix = tvec.fit_transform(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383a92bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many features are there? \n",
    "\n",
    "# is it the same as the count vectorizer with the same hyperparameter values?\n",
    "tvec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b08b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put in a dataframe and view it\n",
    "tv_matrix.todense()\n",
    "\n",
    "df = pd.DataFrame(data = tv_matrix.todense(), columns = tvec.get_feature_names())\n",
    "# What kind of values are stored now?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1dcc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e1e57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the top occurring features. Are they the same as before?\n",
    "df.sum().sort_values(ascending=False).head(20).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61facac",
   "metadata": {},
   "source": [
    "Are the results different? Try with different hyperparameter values. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
