{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "254db1e7",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# NLP Part 1: Language Data Pre-Processing\n",
    "\n",
    "\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "1. The Natural Language Toolkit (NLTK) \n",
    "2. Define and implement tokenizing, lemmatizing, stemming, part-of-speech tagging and named entity recognition.\n",
    "3. Preprocess text data by removing stopwords\n",
    "4. Implement sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d77c8c",
   "metadata": {},
   "source": [
    "Before we get started, you will need to install the Natural Language Toolkit, known as [NLTK](https://www.nltk.org/install.html). If it is already installed you will get a `Requirement already satisfied` message.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c15fb757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in d:\\programdata\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in d:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in d:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in d:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in d:\\programdata\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e7b14e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1affe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "# Download all the packages from nltk\n",
    "#nltk.download('all')\n",
    "# Alternatively, choose package to download from the window that pops up.\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67a8be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd       \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bc66e2",
   "metadata": {},
   "source": [
    "# Pre-Processing \n",
    "\n",
    "When dealing with text data, there are common pre-processing steps that NLTK can help us to do:\n",
    "- Remove special characters\n",
    "- Tokenizing\n",
    "- Lemmatizing/Stemming\n",
    "- Stop word removal\n",
    "\n",
    "Let's say we have the following text that we want to be able to identify as spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7359d600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define spam text.\n",
    "spam = 'Hello,\\nI saw your contact information on LinkedIn. I have carefully read through your profile and you seem to have an outstanding personality. This is one major reason why I am in contact with you. My name is Mr. Valery Grayfer Chairman of the Board of Directors of PJSC \"LUKOIL\". I am 86 years old and I was diagnosed with cancer 2 years ago. I will be going in for an operation later this week. I decided to WILL/Donate the sum of 8,750,000.00 Euros(Eight Million Seven Hundred And Fifty Thousand Euros Only. If you would like to receive this donation please reply by contacting me before I am operated on.'\n",
    "\n",
    "print(spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f9c463",
   "metadata": {},
   "source": [
    "## Tokenizing\n",
    "\n",
    "NLTK provides functions to split text into individual words or sentences, called 'tokens'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac3e89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize into sentences\n",
    "sent_tokenize(spam.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c179d568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize into words\n",
    "word_tokenize(spam.lower())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfacc72",
   "metadata": {},
   "source": [
    "You might notice that  `word_tokenize()` considers special characters as words too. To overcome this, we can instantiate a  `RegexpTokenizer` to specify that we want words containing digits or characters only using the Regex `\\w+`\n",
    "\n",
    "For more on Regular Expressions (and to test a regex) see https://regex101.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40984d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \\w matches any word character (equivalent to [a-zA-Z0-9_]), + to repeat the match\n",
    "# So this tokenizer finds all the words containing alphabets, digits or _\n",
    "tokenizer = RegexpTokenizer(r'\\w+') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05219622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the regular expression tokenizer that has been instantiated \n",
    "spam_tokens = tokenizer.tokenize(spam.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4c11ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What happened with the numeric value?\n",
    "spam_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e4a491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for digits in each of the tokens\n",
    "[(re.findall('\\d+', i), i) for i in spam_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf25799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate tokenizer to include digits attached to , or .\n",
    "tokenizer_1 = RegexpTokenizer('\\d[\\d,.]+|\\w+')\n",
    "tokenizer_1.tokenize(spam.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b44f8d7",
   "metadata": {},
   "source": [
    "## Lemmatizing & Stemming\n",
    "\n",
    "- \"He is *running* really fast!\"\n",
    "- \"He *ran* the race.\"\n",
    "- \"He *runs* a five-minute mile.\"\n",
    "\n",
    "If we wanted a computer to interpret these sentences, I might count up how many times I see each word. The computer will treat words like \"running,\" \"ran,\" and \"runs\" differently... but they mean very similar things (in this context)!\n",
    "\n",
    "**Lemmatizing** and **stemming** are two forms of shortening words so we can combine similar forms of the same word.\n",
    "\n",
    "When we \"**lemmatize**\" data, we take words and attempt to return their *lemma*, or the base/dictionary form of a word.\n",
    "\n",
    "How do we know what the base form is? Fortunately we don't have to create a dictionary, NLTK uses the publicly available [WordNet](https://wordnet.princeton.edu/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd13535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate lemmatizer.\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44003a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize tokens.\n",
    "tokens_lem = [lemmatizer.lemmatize(i) for i in spam_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcab1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare tokens to lemmatized version.\n",
    "# zip() creates pairs of tuples from each list\n",
    "list(zip(spam_tokens, tokens_lem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a265a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print only those lemmatized tokens that are different.\n",
    "for i in range(len(spam_tokens)):\n",
    "    if spam_tokens[i] != tokens_lem[i]:\n",
    "        print((spam_tokens[i], tokens_lem[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d38402f",
   "metadata": {},
   "source": [
    "Lemmatizing is usually the more correct and precise way of handling things from a grammatical point of view, but also might not have much of an effect.\n",
    "\n",
    "We can also do this on individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d98639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize the word \"computers.\"\n",
    "lemmatizer.lemmatize(\"computers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14dc1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer.lemmatize(\"computer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448429c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer.lemmatize(\"computation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c27576",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer.lemmatize(\"computationally\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec0d00b",
   "metadata": {},
   "source": [
    "When we \"**stem**\" data, we take words and attempt to return a base form of the word. It tends to be cruder than using lemmatization. There's a [method developed by Porter in 1980](https://www.cs.toronto.edu/~frank/csc2501/Readings/R2_Porter/Porter-1980.pdf) that explains the algorithm used below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bdc6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate PorterStemmer.\n",
    "p_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f161cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stem tokens.\n",
    "stem_spam = [p_stemmer.stem(i) for i in spam_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ded440e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare tokens to stemmed version.\n",
    "list(zip(spam_tokens, stem_spam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1114ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print only those stemmed tokens that are different.\n",
    "\n",
    "[(spam_tokens[i], stem_spam[i]) for i in range(len(spam_tokens)) if spam_tokens[i] != stem_spam[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afb18e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stem the word \"computers\"\n",
    "p_stemmer.stem(\"computers\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380b1a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stem the word \"computer.\"\n",
    "p_stemmer.stem(\"computer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfb4d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Stem the word \"computation.\"\n",
    "p_stemmer.stem(\"computation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59102fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Stem the word \"computationally.\"\n",
    "p_stemmer.stem(\"computationally\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2ebf6b",
   "metadata": {},
   "source": [
    "## Part of Speech Tagging\n",
    "\n",
    "Another task in NLP is to tag the different parts of speech. This can be done using NLTK's `pos_tag`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a57620",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text = tokenizer.tokenize(\"Taylor Swift's concert The Eras Tour will be held at the Singapore National Stadium in Singapore on 2,3,4 and 7,8,9 March 2024\")\n",
    "\n",
    "\n",
    "pos_tag_list = pos_tag(text)\n",
    "print(pos_tag_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9114ab94",
   "metadata": {},
   "source": [
    "NLTK has tagged the tokens as various parts of speech:\n",
    " \n",
    "- NN: noun \n",
    "- IN: preposition and conjunction \n",
    "- CD: digit \n",
    "- VBN, VBG : verbs\n",
    "- JJ: adjective \n",
    "- PRP: pronoun\n",
    "- MD: modal \n",
    "- CC: conjunction\n",
    "-etc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dc2bf9",
   "metadata": {},
   "source": [
    "## Named Entity Recognition\n",
    "\n",
    "Another NLP task is to perform named entity recognition, ie to identify locations, people and organizations. This is done by passing the tagged parts of speech to the `nltk ne_chunk` module.\n",
    "The nltk chunk module will parse the parts of speech to identify tokens which are probably named entities. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d49fe78",
   "metadata": {},
   "outputs": [],
   "source": [
    "namedEnt = ne_chunk(pos_tag_list, binary=False)\n",
    "namedEnt.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83afdd1",
   "metadata": {},
   "source": [
    "## Stop Word Removal\n",
    "\n",
    "The following quote has had stop words (and punctuation) removed:\n",
    "\n",
    "\"Answer great question life universe everything said deep thought said deep thought paused forty two said deep thought infinite majesty calm.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4405b1e4",
   "metadata": {},
   "source": [
    "<details><summary>What book is the above sentence from?</summary>\n",
    "\n",
    "The Hitchhiker's Guide to the Galaxy!\n",
    "    \n",
    "![](../images/hgg.jpg)\n",
    "    \n",
    "The original quote reads:  \n",
    "...\"The Answer to the Great Question...\"  \n",
    "\"Yes..!\"  \n",
    "\"Of Life, the Universe and Everything...\" said Deep Thought.  \n",
    "\"Yes...!\"  \n",
    "\"Is...\" said Deep Thought, and paused.  \n",
    "\"Yes...!\"  \n",
    "\"Is...\"  \n",
    "\"Yes...!!!...?\"  \n",
    "\"Forty-two,\" said Deep Thought, with infinite majesty and calm.‚Äù\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f94e5f",
   "metadata": {},
   "source": [
    "<details><summary>If you were familiar with the book, how did you know what book the sentence was from?</summary>\n",
    "\n",
    "Removing stop words did not remove key identifying words such as \"life\", \"universe\", \"everything\", and \"forty-two\".\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902725d3",
   "metadata": {},
   "source": [
    "<details><summary>Based on this, how would you define stop words?</summary>\n",
    "\n",
    "Stop words are words that have little to no significance or meaning. They are common words that only add to the grammatical structure and flow of the sentence, so it is still relatively easy to identify the contents of sentences without stop words.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb59d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print English stopwords.\n",
    "print(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7f5c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords from \"spam_tokens.\"\n",
    "no_stop_words = [token for token in spam_tokens if token not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016bae41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check it\n",
    "print(no_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d97073",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Sentiment Analysis\n",
    "\n",
    "![](../images/sent.jpeg)\n",
    "\n",
    "[Sentiment analysis](https://www.kdnuggets.com/2018/08/emotion-sentiment-analysis-practitioners-guide-nlp-5.html) is an area of natural language processing in which we seek to classify text as having positive or negative emotion.\n",
    "\n",
    "Let's build a simple function that can classify text as either having positive or negative sentiment.\n",
    "\n",
    "What words tell us whether certain text is positive?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2731ded8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's come up with a list of positive and negative words we might observe. \n",
    "# Suggest more!!\n",
    "positive_words = ['love', 'good', 'great']\n",
    "negative_words = ['garbage', 'sad', 'bad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8164f939",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_sentiment(text):\n",
    "    # Instantiate tokenizer.\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')  \n",
    "    # Tokenize text.\n",
    "    tokens = tokenizer.tokenize(text.lower()) \n",
    "    # Instantiate stemmer.\n",
    "    p_stemmer = PorterStemmer()\n",
    "    # Stem words.\n",
    "    stemmed_words = [p_stemmer.stem(i) for i in tokens]\n",
    "    # Stem our positive/negative words.\n",
    "    positive_stems = [p_stemmer.stem(i) for i in positive_words]\n",
    "    negative_stems = [p_stemmer.stem(i) for i in negative_words]\n",
    "\n",
    "    # Count \"positive\" words.\n",
    "    positive_count = sum([1 for i in stemmed_words if i in positive_stems])\n",
    "    # Count \"negative\" words\n",
    "    negative_count = sum([1 for i in stemmed_words if i in negative_stems])\n",
    "    # Calculate Sentiment Percentage \n",
    "    return round((positive_count - negative_count) / len(tokens), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f1367e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run our sentiment analyzer \n",
    "simple_sentiment(\"I love programming!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a5b013",
   "metadata": {},
   "source": [
    "<details><summary> What are some shortcomings of this method? </summary>\n",
    "\n",
    "- Primarily, we're limited to the positive/negative words we came up with.\n",
    "- If someone wrote \"not good\" or \"not bad,\" our sentiment function would probably treat \"not good\" as positive or neutral... but it's probably supposed to mean negative!\n",
    "- The ordering of the words doesn't matter here, which is not how language generally works.\n",
    "- We haven't corrected for misspellings.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daac202a",
   "metadata": {},
   "source": [
    "There are a couple of ways to proceed with sentiment analysis:\n",
    "\n",
    "1. If you have already-labeled data, you can build a supervised learning model.\n",
    "2. If you don't have labeled data, you can use a Lexicon (dictionary) that has already been built/trained for sentiment analysis.\n",
    "    - There are a bunch of these and which to use depends on your purpose/data. Here are just a few that are available:\n",
    "        - AFINN lexicon\n",
    "        - MPQA subjectivity lexicon\n",
    "        - SentiWordNet\n",
    "        - VADER lexicon\n",
    "\n",
    "We will use the [VADER](https://www.nltk.org/_modules/nltk/sentiment/vader.html) (Valence Aware Dictionary and sEntiment Reasoner) lexicon to analyze the sentiments of our reviews.\n",
    "\n",
    "VADER has a basic lexicon for positive and negative sentiments, including emoticons and negation ('didn't like'), but also considers intensity in terms of use of CAPs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32b1c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Sentiment Intensity Analyzer from nltk.sentiment.vader\n",
    "sent = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310689c2",
   "metadata": {},
   "source": [
    "VADER's `polarity_scores` takes in a string and returns a dictionary of scores in each of four categories:\n",
    "\n",
    "- negative\n",
    "- neutral\n",
    "- positive\n",
    "- compound, which calculates a score based on the above three. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf30931",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent.polarity_scores('This is FANTASTIC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63e6992",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent.polarity_scores('I don\"t like programming at all and I hate Python especially')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a8c1b4",
   "metadata": {},
   "source": [
    "Let's try analyzing the sentiment of IMDb movie reviews. The data is from [Kaggle](https://www.kaggle.com/c/word2vec-nlp-tutorial#part-1-for-beginners-bag-of-words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3553a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in training data.\n",
    "reviews = pd.read_csv(\"data/labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be0a3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d553ab65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine a review.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae54706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the sent.polarity_scores() to find the sentiment of the review.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225aa914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does this match the sentiment given in the training data?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1d2939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the scores to all the reviews\n",
    "reviews['scores'] = reviews['review'].apply(sent.polarity_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7706f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract only the compound score\n",
    "reviews['compound']  = reviews['scores'].apply(lambda score_dict: score_dict['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bee519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for the first 10 reviews\n",
    "reviews.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe3c90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a look at one of the reviews that is different from the labelled sentiment\n",
    "\n",
    "\n",
    "# What sentiment would YOU assign?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3307dfd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
