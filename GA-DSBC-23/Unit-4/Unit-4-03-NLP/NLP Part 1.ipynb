{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "254db1e7",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# NLP Part 1: Language Data Pre-Processing\n",
    "\n",
    "\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "1. The Natural Language Toolkit (NLTK) \n",
    "2. Define and implement tokenizing, lemmatizing, stemming, part-of-speech tagging and named entity recognition.\n",
    "3. Preprocess text data by removing stopwords\n",
    "4. Implement sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d77c8c",
   "metadata": {},
   "source": [
    "Before we get started, you will need to install the Natural Language Toolkit, known as [NLTK](https://www.nltk.org/install.html). If it is already installed you will get a `Requirement already satisfied` message.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c15fb757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in d:\\programdata\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in d:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in d:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in d:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in d:\\programdata\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e7b14e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc1affe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download all the packages from nltk\n",
    "#nltk.download('all')\n",
    "# Alternatively, choose package to download from the window that pops up.\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b67a8be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd       \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bc66e2",
   "metadata": {},
   "source": [
    "# Pre-Processing \n",
    "\n",
    "When dealing with text data, there are common pre-processing steps that NLTK can help us to do:\n",
    "- Remove special characters\n",
    "- Tokenizing\n",
    "- Lemmatizing/Stemming\n",
    "- Stop word removal\n",
    "\n",
    "Let's say we have the following text that we want to be able to identify as spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7359d600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello,\n",
      "I saw your contact information on LinkedIn. I have carefully read through your profile and you seem to have an outstanding personality. This is one major reason why I am in contact with you. My name is Mr. Valery Grayfer Chairman of the Board of Directors of PJSC \"LUKOIL\". I am 86 years old and I was diagnosed with cancer 2 years ago. I will be going in for an operation later this week. I decided to WILL/Donate the sum of 8,750,000.00 Euros(Eight Million Seven Hundred And Fifty Thousand Euros Only. If you would like to receive this donation please reply by contacting me before I am operated on.\n"
     ]
    }
   ],
   "source": [
    "# Define spam text.\n",
    "spam = 'Hello,\\nI saw your contact information on LinkedIn. I have carefully read through your profile and you seem to have an outstanding personality. This is one major reason why I am in contact with you. My name is Mr. Valery Grayfer Chairman of the Board of Directors of PJSC \"LUKOIL\". I am 86 years old and I was diagnosed with cancer 2 years ago. I will be going in for an operation later this week. I decided to WILL/Donate the sum of 8,750,000.00 Euros(Eight Million Seven Hundred And Fifty Thousand Euros Only. If you would like to receive this donation please reply by contacting me before I am operated on.'\n",
    "\n",
    "print(spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f9c463",
   "metadata": {},
   "source": [
    "## Tokenizing\n",
    "\n",
    "NLTK provides functions to split text into individual words or sentences, called 'tokens'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ac3e89e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello,\\ni saw your contact information on linkedin.',\n",
       " 'i have carefully read through your profile and you seem to have an outstanding personality.',\n",
       " 'this is one major reason why i am in contact with you.',\n",
       " 'my name is mr. valery grayfer chairman of the board of directors of pjsc \"lukoil\".',\n",
       " 'i am 86 years old and i was diagnosed with cancer 2 years ago.',\n",
       " 'i will be going in for an operation later this week.',\n",
       " 'i decided to will/donate the sum of 8,750,000.00 euros(eight million seven hundred and fifty thousand euros only.',\n",
       " 'if you would like to receive this donation please reply by contacting me before i am operated on.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize into sentences\n",
    "sent_tokenize(spam.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c179d568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " ',',\n",
       " 'i',\n",
       " 'saw',\n",
       " 'your',\n",
       " 'contact',\n",
       " 'information',\n",
       " 'on',\n",
       " 'linkedin',\n",
       " '.',\n",
       " 'i',\n",
       " 'have',\n",
       " 'carefully',\n",
       " 'read',\n",
       " 'through',\n",
       " 'your',\n",
       " 'profile',\n",
       " 'and',\n",
       " 'you',\n",
       " 'seem',\n",
       " 'to',\n",
       " 'have',\n",
       " 'an',\n",
       " 'outstanding',\n",
       " 'personality',\n",
       " '.',\n",
       " 'this',\n",
       " 'is',\n",
       " 'one',\n",
       " 'major',\n",
       " 'reason',\n",
       " 'why',\n",
       " 'i',\n",
       " 'am',\n",
       " 'in',\n",
       " 'contact',\n",
       " 'with',\n",
       " 'you',\n",
       " '.',\n",
       " 'my',\n",
       " 'name',\n",
       " 'is',\n",
       " 'mr.',\n",
       " 'valery',\n",
       " 'grayfer',\n",
       " 'chairman',\n",
       " 'of',\n",
       " 'the',\n",
       " 'board',\n",
       " 'of',\n",
       " 'directors',\n",
       " 'of',\n",
       " 'pjsc',\n",
       " '``',\n",
       " 'lukoil',\n",
       " \"''\",\n",
       " '.',\n",
       " 'i',\n",
       " 'am',\n",
       " '86',\n",
       " 'years',\n",
       " 'old',\n",
       " 'and',\n",
       " 'i',\n",
       " 'was',\n",
       " 'diagnosed',\n",
       " 'with',\n",
       " 'cancer',\n",
       " '2',\n",
       " 'years',\n",
       " 'ago',\n",
       " '.',\n",
       " 'i',\n",
       " 'will',\n",
       " 'be',\n",
       " 'going',\n",
       " 'in',\n",
       " 'for',\n",
       " 'an',\n",
       " 'operation',\n",
       " 'later',\n",
       " 'this',\n",
       " 'week',\n",
       " '.',\n",
       " 'i',\n",
       " 'decided',\n",
       " 'to',\n",
       " 'will/donate',\n",
       " 'the',\n",
       " 'sum',\n",
       " 'of',\n",
       " '8,750,000.00',\n",
       " 'euros',\n",
       " '(',\n",
       " 'eight',\n",
       " 'million',\n",
       " 'seven',\n",
       " 'hundred',\n",
       " 'and',\n",
       " 'fifty',\n",
       " 'thousand',\n",
       " 'euros',\n",
       " 'only',\n",
       " '.',\n",
       " 'if',\n",
       " 'you',\n",
       " 'would',\n",
       " 'like',\n",
       " 'to',\n",
       " 'receive',\n",
       " 'this',\n",
       " 'donation',\n",
       " 'please',\n",
       " 'reply',\n",
       " 'by',\n",
       " 'contacting',\n",
       " 'me',\n",
       " 'before',\n",
       " 'i',\n",
       " 'am',\n",
       " 'operated',\n",
       " 'on',\n",
       " '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize into words\n",
    "word_tokenize(spam.lower())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfacc72",
   "metadata": {},
   "source": [
    "You might notice that  `word_tokenize()` considers special characters as words too. To overcome this, we can instantiate a  `RegexpTokenizer` to specify that we want words containing digits or characters only using the Regex `\\w+`\n",
    "\n",
    "For more on Regular Expressions (and to test a regex) see https://regex101.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40984d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \\w matches any word character (equivalent to [a-zA-Z0-9_]), + to repeat the match\n",
    "# So this tokenizer finds all the words containing alphabets, digits or _\n",
    "tokenizer = RegexpTokenizer(r'\\w+') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05219622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the regular expression tokenizer that has been instantiated \n",
    "spam_tokens = tokenizer.tokenize(spam.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c4c11ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " 'i',\n",
       " 'saw',\n",
       " 'your',\n",
       " 'contact',\n",
       " 'information',\n",
       " 'on',\n",
       " 'linkedin',\n",
       " 'i',\n",
       " 'have',\n",
       " 'carefully',\n",
       " 'read',\n",
       " 'through',\n",
       " 'your',\n",
       " 'profile',\n",
       " 'and',\n",
       " 'you',\n",
       " 'seem',\n",
       " 'to',\n",
       " 'have',\n",
       " 'an',\n",
       " 'outstanding',\n",
       " 'personality',\n",
       " 'this',\n",
       " 'is',\n",
       " 'one',\n",
       " 'major',\n",
       " 'reason',\n",
       " 'why',\n",
       " 'i',\n",
       " 'am',\n",
       " 'in',\n",
       " 'contact',\n",
       " 'with',\n",
       " 'you',\n",
       " 'my',\n",
       " 'name',\n",
       " 'is',\n",
       " 'mr',\n",
       " 'valery',\n",
       " 'grayfer',\n",
       " 'chairman',\n",
       " 'of',\n",
       " 'the',\n",
       " 'board',\n",
       " 'of',\n",
       " 'directors',\n",
       " 'of',\n",
       " 'pjsc',\n",
       " 'lukoil',\n",
       " 'i',\n",
       " 'am',\n",
       " '86',\n",
       " 'years',\n",
       " 'old',\n",
       " 'and',\n",
       " 'i',\n",
       " 'was',\n",
       " 'diagnosed',\n",
       " 'with',\n",
       " 'cancer',\n",
       " '2',\n",
       " 'years',\n",
       " 'ago',\n",
       " 'i',\n",
       " 'will',\n",
       " 'be',\n",
       " 'going',\n",
       " 'in',\n",
       " 'for',\n",
       " 'an',\n",
       " 'operation',\n",
       " 'later',\n",
       " 'this',\n",
       " 'week',\n",
       " 'i',\n",
       " 'decided',\n",
       " 'to',\n",
       " 'will',\n",
       " 'donate',\n",
       " 'the',\n",
       " 'sum',\n",
       " 'of',\n",
       " '8',\n",
       " '750',\n",
       " '000',\n",
       " '00',\n",
       " 'euros',\n",
       " 'eight',\n",
       " 'million',\n",
       " 'seven',\n",
       " 'hundred',\n",
       " 'and',\n",
       " 'fifty',\n",
       " 'thousand',\n",
       " 'euros',\n",
       " 'only',\n",
       " 'if',\n",
       " 'you',\n",
       " 'would',\n",
       " 'like',\n",
       " 'to',\n",
       " 'receive',\n",
       " 'this',\n",
       " 'donation',\n",
       " 'please',\n",
       " 'reply',\n",
       " 'by',\n",
       " 'contacting',\n",
       " 'me',\n",
       " 'before',\n",
       " 'i',\n",
       " 'am',\n",
       " 'operated',\n",
       " 'on']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What happened with the numeric value?\n",
    "spam_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79e4a491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([], 'hello'),\n",
       " ([], 'i'),\n",
       " ([], 'saw'),\n",
       " ([], 'your'),\n",
       " ([], 'contact'),\n",
       " ([], 'information'),\n",
       " ([], 'on'),\n",
       " ([], 'linkedin'),\n",
       " ([], 'i'),\n",
       " ([], 'have'),\n",
       " ([], 'carefully'),\n",
       " ([], 'read'),\n",
       " ([], 'through'),\n",
       " ([], 'your'),\n",
       " ([], 'profile'),\n",
       " ([], 'and'),\n",
       " ([], 'you'),\n",
       " ([], 'seem'),\n",
       " ([], 'to'),\n",
       " ([], 'have'),\n",
       " ([], 'an'),\n",
       " ([], 'outstanding'),\n",
       " ([], 'personality'),\n",
       " ([], 'this'),\n",
       " ([], 'is'),\n",
       " ([], 'one'),\n",
       " ([], 'major'),\n",
       " ([], 'reason'),\n",
       " ([], 'why'),\n",
       " ([], 'i'),\n",
       " ([], 'am'),\n",
       " ([], 'in'),\n",
       " ([], 'contact'),\n",
       " ([], 'with'),\n",
       " ([], 'you'),\n",
       " ([], 'my'),\n",
       " ([], 'name'),\n",
       " ([], 'is'),\n",
       " ([], 'mr'),\n",
       " ([], 'valery'),\n",
       " ([], 'grayfer'),\n",
       " ([], 'chairman'),\n",
       " ([], 'of'),\n",
       " ([], 'the'),\n",
       " ([], 'board'),\n",
       " ([], 'of'),\n",
       " ([], 'directors'),\n",
       " ([], 'of'),\n",
       " ([], 'pjsc'),\n",
       " ([], 'lukoil'),\n",
       " ([], 'i'),\n",
       " ([], 'am'),\n",
       " (['86'], '86'),\n",
       " ([], 'years'),\n",
       " ([], 'old'),\n",
       " ([], 'and'),\n",
       " ([], 'i'),\n",
       " ([], 'was'),\n",
       " ([], 'diagnosed'),\n",
       " ([], 'with'),\n",
       " ([], 'cancer'),\n",
       " (['2'], '2'),\n",
       " ([], 'years'),\n",
       " ([], 'ago'),\n",
       " ([], 'i'),\n",
       " ([], 'will'),\n",
       " ([], 'be'),\n",
       " ([], 'going'),\n",
       " ([], 'in'),\n",
       " ([], 'for'),\n",
       " ([], 'an'),\n",
       " ([], 'operation'),\n",
       " ([], 'later'),\n",
       " ([], 'this'),\n",
       " ([], 'week'),\n",
       " ([], 'i'),\n",
       " ([], 'decided'),\n",
       " ([], 'to'),\n",
       " ([], 'will'),\n",
       " ([], 'donate'),\n",
       " ([], 'the'),\n",
       " ([], 'sum'),\n",
       " ([], 'of'),\n",
       " (['8'], '8'),\n",
       " (['750'], '750'),\n",
       " (['000'], '000'),\n",
       " (['00'], '00'),\n",
       " ([], 'euros'),\n",
       " ([], 'eight'),\n",
       " ([], 'million'),\n",
       " ([], 'seven'),\n",
       " ([], 'hundred'),\n",
       " ([], 'and'),\n",
       " ([], 'fifty'),\n",
       " ([], 'thousand'),\n",
       " ([], 'euros'),\n",
       " ([], 'only'),\n",
       " ([], 'if'),\n",
       " ([], 'you'),\n",
       " ([], 'would'),\n",
       " ([], 'like'),\n",
       " ([], 'to'),\n",
       " ([], 'receive'),\n",
       " ([], 'this'),\n",
       " ([], 'donation'),\n",
       " ([], 'please'),\n",
       " ([], 'reply'),\n",
       " ([], 'by'),\n",
       " ([], 'contacting'),\n",
       " ([], 'me'),\n",
       " ([], 'before'),\n",
       " ([], 'i'),\n",
       " ([], 'am'),\n",
       " ([], 'operated'),\n",
       " ([], 'on')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look for digits in each of the tokens\n",
    "[(re.findall('\\d+', i), i) for i in spam_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bf25799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " 'i',\n",
       " 'saw',\n",
       " 'your',\n",
       " 'contact',\n",
       " 'information',\n",
       " 'on',\n",
       " 'linkedin',\n",
       " 'i',\n",
       " 'have',\n",
       " 'carefully',\n",
       " 'read',\n",
       " 'through',\n",
       " 'your',\n",
       " 'profile',\n",
       " 'and',\n",
       " 'you',\n",
       " 'seem',\n",
       " 'to',\n",
       " 'have',\n",
       " 'an',\n",
       " 'outstanding',\n",
       " 'personality',\n",
       " 'this',\n",
       " 'is',\n",
       " 'one',\n",
       " 'major',\n",
       " 'reason',\n",
       " 'why',\n",
       " 'i',\n",
       " 'am',\n",
       " 'in',\n",
       " 'contact',\n",
       " 'with',\n",
       " 'you',\n",
       " 'my',\n",
       " 'name',\n",
       " 'is',\n",
       " 'mr',\n",
       " 'valery',\n",
       " 'grayfer',\n",
       " 'chairman',\n",
       " 'of',\n",
       " 'the',\n",
       " 'board',\n",
       " 'of',\n",
       " 'directors',\n",
       " 'of',\n",
       " 'pjsc',\n",
       " 'lukoil',\n",
       " 'i',\n",
       " 'am',\n",
       " '86',\n",
       " 'years',\n",
       " 'old',\n",
       " 'and',\n",
       " 'i',\n",
       " 'was',\n",
       " 'diagnosed',\n",
       " 'with',\n",
       " 'cancer',\n",
       " '2',\n",
       " 'years',\n",
       " 'ago',\n",
       " 'i',\n",
       " 'will',\n",
       " 'be',\n",
       " 'going',\n",
       " 'in',\n",
       " 'for',\n",
       " 'an',\n",
       " 'operation',\n",
       " 'later',\n",
       " 'this',\n",
       " 'week',\n",
       " 'i',\n",
       " 'decided',\n",
       " 'to',\n",
       " 'will',\n",
       " 'donate',\n",
       " 'the',\n",
       " 'sum',\n",
       " 'of',\n",
       " '8,750,000.00',\n",
       " 'euros',\n",
       " 'eight',\n",
       " 'million',\n",
       " 'seven',\n",
       " 'hundred',\n",
       " 'and',\n",
       " 'fifty',\n",
       " 'thousand',\n",
       " 'euros',\n",
       " 'only',\n",
       " 'if',\n",
       " 'you',\n",
       " 'would',\n",
       " 'like',\n",
       " 'to',\n",
       " 'receive',\n",
       " 'this',\n",
       " 'donation',\n",
       " 'please',\n",
       " 'reply',\n",
       " 'by',\n",
       " 'contacting',\n",
       " 'me',\n",
       " 'before',\n",
       " 'i',\n",
       " 'am',\n",
       " 'operated',\n",
       " 'on']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate tokenizer to include digits attached to , or .\n",
    "tokenizer_1 = RegexpTokenizer('\\d[\\d,.]+|\\w+')\n",
    "tokenizer_1.tokenize(spam.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b44f8d7",
   "metadata": {},
   "source": [
    "## Lemmatizing & Stemming\n",
    "\n",
    "- \"He is *running* really fast!\"\n",
    "- \"He *ran* the race.\"\n",
    "- \"He *runs* a five-minute mile.\"\n",
    "\n",
    "If we wanted a computer to interpret these sentences, I might count up how many times I see each word. The computer will treat words like \"running,\" \"ran,\" and \"runs\" differently... but they mean very similar things (in this context)!\n",
    "\n",
    "**Lemmatizing** and **stemming** are two forms of shortening words so we can combine similar forms of the same word.\n",
    "\n",
    "When we \"**lemmatize**\" data, we take words and attempt to return their *lemma*, or the base/dictionary form of a word.\n",
    "\n",
    "How do we know what the base form is? Fortunately we don't have to create a dictionary, NLTK uses the publicly available [WordNet](https://wordnet.princeton.edu/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bd13535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate lemmatizer.\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44003a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize tokens.\n",
    "tokens_lem = [lemmatizer.lemmatize(i) for i in spam_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8dcab1c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hello', 'hello'),\n",
       " ('i', 'i'),\n",
       " ('saw', 'saw'),\n",
       " ('your', 'your'),\n",
       " ('contact', 'contact'),\n",
       " ('information', 'information'),\n",
       " ('on', 'on'),\n",
       " ('linkedin', 'linkedin'),\n",
       " ('i', 'i'),\n",
       " ('have', 'have'),\n",
       " ('carefully', 'carefully'),\n",
       " ('read', 'read'),\n",
       " ('through', 'through'),\n",
       " ('your', 'your'),\n",
       " ('profile', 'profile'),\n",
       " ('and', 'and'),\n",
       " ('you', 'you'),\n",
       " ('seem', 'seem'),\n",
       " ('to', 'to'),\n",
       " ('have', 'have'),\n",
       " ('an', 'an'),\n",
       " ('outstanding', 'outstanding'),\n",
       " ('personality', 'personality'),\n",
       " ('this', 'this'),\n",
       " ('is', 'is'),\n",
       " ('one', 'one'),\n",
       " ('major', 'major'),\n",
       " ('reason', 'reason'),\n",
       " ('why', 'why'),\n",
       " ('i', 'i'),\n",
       " ('am', 'am'),\n",
       " ('in', 'in'),\n",
       " ('contact', 'contact'),\n",
       " ('with', 'with'),\n",
       " ('you', 'you'),\n",
       " ('my', 'my'),\n",
       " ('name', 'name'),\n",
       " ('is', 'is'),\n",
       " ('mr', 'mr'),\n",
       " ('valery', 'valery'),\n",
       " ('grayfer', 'grayfer'),\n",
       " ('chairman', 'chairman'),\n",
       " ('of', 'of'),\n",
       " ('the', 'the'),\n",
       " ('board', 'board'),\n",
       " ('of', 'of'),\n",
       " ('directors', 'director'),\n",
       " ('of', 'of'),\n",
       " ('pjsc', 'pjsc'),\n",
       " ('lukoil', 'lukoil'),\n",
       " ('i', 'i'),\n",
       " ('am', 'am'),\n",
       " ('86', '86'),\n",
       " ('years', 'year'),\n",
       " ('old', 'old'),\n",
       " ('and', 'and'),\n",
       " ('i', 'i'),\n",
       " ('was', 'wa'),\n",
       " ('diagnosed', 'diagnosed'),\n",
       " ('with', 'with'),\n",
       " ('cancer', 'cancer'),\n",
       " ('2', '2'),\n",
       " ('years', 'year'),\n",
       " ('ago', 'ago'),\n",
       " ('i', 'i'),\n",
       " ('will', 'will'),\n",
       " ('be', 'be'),\n",
       " ('going', 'going'),\n",
       " ('in', 'in'),\n",
       " ('for', 'for'),\n",
       " ('an', 'an'),\n",
       " ('operation', 'operation'),\n",
       " ('later', 'later'),\n",
       " ('this', 'this'),\n",
       " ('week', 'week'),\n",
       " ('i', 'i'),\n",
       " ('decided', 'decided'),\n",
       " ('to', 'to'),\n",
       " ('will', 'will'),\n",
       " ('donate', 'donate'),\n",
       " ('the', 'the'),\n",
       " ('sum', 'sum'),\n",
       " ('of', 'of'),\n",
       " ('8', '8'),\n",
       " ('750', '750'),\n",
       " ('000', '000'),\n",
       " ('00', '00'),\n",
       " ('euros', 'euro'),\n",
       " ('eight', 'eight'),\n",
       " ('million', 'million'),\n",
       " ('seven', 'seven'),\n",
       " ('hundred', 'hundred'),\n",
       " ('and', 'and'),\n",
       " ('fifty', 'fifty'),\n",
       " ('thousand', 'thousand'),\n",
       " ('euros', 'euro'),\n",
       " ('only', 'only'),\n",
       " ('if', 'if'),\n",
       " ('you', 'you'),\n",
       " ('would', 'would'),\n",
       " ('like', 'like'),\n",
       " ('to', 'to'),\n",
       " ('receive', 'receive'),\n",
       " ('this', 'this'),\n",
       " ('donation', 'donation'),\n",
       " ('please', 'please'),\n",
       " ('reply', 'reply'),\n",
       " ('by', 'by'),\n",
       " ('contacting', 'contacting'),\n",
       " ('me', 'me'),\n",
       " ('before', 'before'),\n",
       " ('i', 'i'),\n",
       " ('am', 'am'),\n",
       " ('operated', 'operated'),\n",
       " ('on', 'on')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare tokens to lemmatized version.\n",
    "# zip() creates pairs of tuples from each list\n",
    "list(zip(spam_tokens, tokens_lem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a265a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('directors', 'director')\n",
      "('years', 'year')\n",
      "('was', 'wa')\n",
      "('years', 'year')\n",
      "('euros', 'euro')\n",
      "('euros', 'euro')\n"
     ]
    }
   ],
   "source": [
    "# Print only those lemmatized tokens that are different.\n",
    "for i in range(len(spam_tokens)):\n",
    "    if spam_tokens[i] != tokens_lem[i]:\n",
    "        print((spam_tokens[i], tokens_lem[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d38402f",
   "metadata": {},
   "source": [
    "Lemmatizing is usually the more correct and precise way of handling things from a grammatical point of view, but also might not have much of an effect.\n",
    "\n",
    "We can also do this on individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6d98639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'computer'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lemmatize the word \"computers.\"\n",
    "lemmatizer.lemmatize(\"computers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a14dc1d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'computer'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"computer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "448429c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'computation'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"computation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4c27576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'computationally'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"computationally\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec0d00b",
   "metadata": {},
   "source": [
    "When we \"**stem**\" data, we take words and attempt to return a base form of the word. It tends to be cruder than using lemmatization. There's a [method developed by Porter in 1980](https://www.cs.toronto.edu/~frank/csc2501/Readings/R2_Porter/Porter-1980.pdf) that explains the algorithm used below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77bdc6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate PorterStemmer.\n",
    "p_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57f161cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stem tokens.\n",
    "stem_spam = [p_stemmer.stem(i) for i in spam_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ded440e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hello', 'hello'),\n",
       " ('i', 'i'),\n",
       " ('saw', 'saw'),\n",
       " ('your', 'your'),\n",
       " ('contact', 'contact'),\n",
       " ('information', 'inform'),\n",
       " ('on', 'on'),\n",
       " ('linkedin', 'linkedin'),\n",
       " ('i', 'i'),\n",
       " ('have', 'have'),\n",
       " ('carefully', 'care'),\n",
       " ('read', 'read'),\n",
       " ('through', 'through'),\n",
       " ('your', 'your'),\n",
       " ('profile', 'profil'),\n",
       " ('and', 'and'),\n",
       " ('you', 'you'),\n",
       " ('seem', 'seem'),\n",
       " ('to', 'to'),\n",
       " ('have', 'have'),\n",
       " ('an', 'an'),\n",
       " ('outstanding', 'outstand'),\n",
       " ('personality', 'person'),\n",
       " ('this', 'thi'),\n",
       " ('is', 'is'),\n",
       " ('one', 'one'),\n",
       " ('major', 'major'),\n",
       " ('reason', 'reason'),\n",
       " ('why', 'whi'),\n",
       " ('i', 'i'),\n",
       " ('am', 'am'),\n",
       " ('in', 'in'),\n",
       " ('contact', 'contact'),\n",
       " ('with', 'with'),\n",
       " ('you', 'you'),\n",
       " ('my', 'my'),\n",
       " ('name', 'name'),\n",
       " ('is', 'is'),\n",
       " ('mr', 'mr'),\n",
       " ('valery', 'valeri'),\n",
       " ('grayfer', 'grayfer'),\n",
       " ('chairman', 'chairman'),\n",
       " ('of', 'of'),\n",
       " ('the', 'the'),\n",
       " ('board', 'board'),\n",
       " ('of', 'of'),\n",
       " ('directors', 'director'),\n",
       " ('of', 'of'),\n",
       " ('pjsc', 'pjsc'),\n",
       " ('lukoil', 'lukoil'),\n",
       " ('i', 'i'),\n",
       " ('am', 'am'),\n",
       " ('86', '86'),\n",
       " ('years', 'year'),\n",
       " ('old', 'old'),\n",
       " ('and', 'and'),\n",
       " ('i', 'i'),\n",
       " ('was', 'wa'),\n",
       " ('diagnosed', 'diagnos'),\n",
       " ('with', 'with'),\n",
       " ('cancer', 'cancer'),\n",
       " ('2', '2'),\n",
       " ('years', 'year'),\n",
       " ('ago', 'ago'),\n",
       " ('i', 'i'),\n",
       " ('will', 'will'),\n",
       " ('be', 'be'),\n",
       " ('going', 'go'),\n",
       " ('in', 'in'),\n",
       " ('for', 'for'),\n",
       " ('an', 'an'),\n",
       " ('operation', 'oper'),\n",
       " ('later', 'later'),\n",
       " ('this', 'thi'),\n",
       " ('week', 'week'),\n",
       " ('i', 'i'),\n",
       " ('decided', 'decid'),\n",
       " ('to', 'to'),\n",
       " ('will', 'will'),\n",
       " ('donate', 'donat'),\n",
       " ('the', 'the'),\n",
       " ('sum', 'sum'),\n",
       " ('of', 'of'),\n",
       " ('8', '8'),\n",
       " ('750', '750'),\n",
       " ('000', '000'),\n",
       " ('00', '00'),\n",
       " ('euros', 'euro'),\n",
       " ('eight', 'eight'),\n",
       " ('million', 'million'),\n",
       " ('seven', 'seven'),\n",
       " ('hundred', 'hundr'),\n",
       " ('and', 'and'),\n",
       " ('fifty', 'fifti'),\n",
       " ('thousand', 'thousand'),\n",
       " ('euros', 'euro'),\n",
       " ('only', 'onli'),\n",
       " ('if', 'if'),\n",
       " ('you', 'you'),\n",
       " ('would', 'would'),\n",
       " ('like', 'like'),\n",
       " ('to', 'to'),\n",
       " ('receive', 'receiv'),\n",
       " ('this', 'thi'),\n",
       " ('donation', 'donat'),\n",
       " ('please', 'pleas'),\n",
       " ('reply', 'repli'),\n",
       " ('by', 'by'),\n",
       " ('contacting', 'contact'),\n",
       " ('me', 'me'),\n",
       " ('before', 'befor'),\n",
       " ('i', 'i'),\n",
       " ('am', 'am'),\n",
       " ('operated', 'oper'),\n",
       " ('on', 'on')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare tokens to stemmed version.\n",
    "list(zip(spam_tokens, stem_spam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd1114ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('information', 'inform'),\n",
       " ('carefully', 'care'),\n",
       " ('profile', 'profil'),\n",
       " ('outstanding', 'outstand'),\n",
       " ('personality', 'person'),\n",
       " ('this', 'thi'),\n",
       " ('why', 'whi'),\n",
       " ('valery', 'valeri'),\n",
       " ('directors', 'director'),\n",
       " ('years', 'year'),\n",
       " ('was', 'wa'),\n",
       " ('diagnosed', 'diagnos'),\n",
       " ('years', 'year'),\n",
       " ('going', 'go'),\n",
       " ('operation', 'oper'),\n",
       " ('this', 'thi'),\n",
       " ('decided', 'decid'),\n",
       " ('donate', 'donat'),\n",
       " ('euros', 'euro'),\n",
       " ('hundred', 'hundr'),\n",
       " ('fifty', 'fifti'),\n",
       " ('euros', 'euro'),\n",
       " ('only', 'onli'),\n",
       " ('receive', 'receiv'),\n",
       " ('this', 'thi'),\n",
       " ('donation', 'donat'),\n",
       " ('please', 'pleas'),\n",
       " ('reply', 'repli'),\n",
       " ('contacting', 'contact'),\n",
       " ('before', 'befor'),\n",
       " ('operated', 'oper')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print only those stemmed tokens that are different.\n",
    "\n",
    "[(spam_tokens[i], stem_spam[i]) for i in range(len(spam_tokens)) if spam_tokens[i] != stem_spam[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3afb18e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'comput'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stem the word \"computers\"\n",
    "p_stemmer.stem(\"computers\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "380b1a8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'comput'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stem the word \"computer.\"\n",
    "p_stemmer.stem(\"computer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8bfb4d4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'comput'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Stem the word \"computation.\"\n",
    "p_stemmer.stem(\"computation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e59102fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'comput'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Stem the word \"computationally.\"\n",
    "p_stemmer.stem(\"computationally\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2ebf6b",
   "metadata": {},
   "source": [
    "## Part of Speech Tagging\n",
    "\n",
    "Another task in NLP is to tag the different parts of speech. This can be done using NLTK's `pos_tag`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b9a57620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Taylor', 'NNP'), ('Swift', 'NNP'), ('s', 'VBD'), ('concert', 'VB'), ('The', 'DT'), ('Eras', 'NNP'), ('Tour', 'NNP'), ('will', 'MD'), ('be', 'VB'), ('held', 'VBN'), ('at', 'IN'), ('the', 'DT'), ('Singapore', 'NNP'), ('National', 'NNP'), ('Stadium', 'NNP'), ('in', 'IN'), ('Singapore', 'NNP'), ('on', 'IN'), ('2', 'CD'), ('3', 'CD'), ('4', 'CD'), ('and', 'CC'), ('7', 'CD'), ('8', 'CD'), ('9', 'CD'), ('March', 'NNP'), ('2024', 'CD')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text = tokenizer.tokenize(\"Taylor Swift's concert The Eras Tour will be held at the Singapore National Stadium in Singapore on 2,3,4 and 7,8,9 March 2024\")\n",
    "\n",
    "\n",
    "pos_tag_list = pos_tag(text)\n",
    "print(pos_tag_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9114ab94",
   "metadata": {},
   "source": [
    "NLTK has tagged the tokens as various parts of speech:\n",
    " \n",
    "- NN: noun \n",
    "- IN: preposition and conjunction \n",
    "- CD: digit \n",
    "- VBN, VBG : verbs\n",
    "- JJ: adjective \n",
    "- PRP: pronoun\n",
    "- MD: modal \n",
    "- CC: conjunction\n",
    "-etc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dc2bf9",
   "metadata": {},
   "source": [
    "## Named Entity Recognition\n",
    "\n",
    "Another NLP task is to perform named entity recognition, ie to identify locations, people and organizations. This is done by passing the tagged parts of speech to the `nltk ne_chunk` module.\n",
    "The nltk chunk module will parse the parts of speech to identify tokens which are probably named entities. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4d49fe78",
   "metadata": {},
   "outputs": [],
   "source": [
    "namedEnt = ne_chunk(pos_tag_list, binary=False)\n",
    "namedEnt.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83afdd1",
   "metadata": {},
   "source": [
    "## Stop Word Removal\n",
    "\n",
    "The following quote has had stop words (and punctuation) removed:\n",
    "\n",
    "\"Answer great question life universe everything said deep thought said deep thought paused forty two said deep thought infinite majesty calm.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4405b1e4",
   "metadata": {},
   "source": [
    "<details><summary>What book is the above sentence from?</summary>\n",
    "\n",
    "The Hitchhiker's Guide to the Galaxy!\n",
    "    \n",
    "![](../images/hgg.jpg)\n",
    "    \n",
    "The original quote reads:  \n",
    "...\"The Answer to the Great Question...\"  \n",
    "\"Yes..!\"  \n",
    "\"Of Life, the Universe and Everything...\" said Deep Thought.  \n",
    "\"Yes...!\"  \n",
    "\"Is...\" said Deep Thought, and paused.  \n",
    "\"Yes...!\"  \n",
    "\"Is...\"  \n",
    "\"Yes...!!!...?\"  \n",
    "\"Forty-two,\" said Deep Thought, with infinite majesty and calm.‚Äù\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f94e5f",
   "metadata": {},
   "source": [
    "<details><summary>If you were familiar with the book, how did you know what book the sentence was from?</summary>\n",
    "\n",
    "Removing stop words did not remove key identifying words such as \"life\", \"universe\", \"everything\", and \"forty-two\".\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902725d3",
   "metadata": {},
   "source": [
    "<details><summary>Based on this, how would you define stop words?</summary>\n",
    "\n",
    "Stop words are words that have little to no significance or meaning. They are common words that only add to the grammatical structure and flow of the sentence, so it is still relatively easy to identify the contents of sentences without stop words.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cdb59d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# Print English stopwords.\n",
    "print(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6d7f5c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords from \"spam_tokens.\"\n",
    "no_stop_words = [token for token in spam_tokens if token not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "016bae41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'saw', 'contact', 'information', 'linkedin', 'carefully', 'read', 'profile', 'seem', 'outstanding', 'personality', 'one', 'major', 'reason', 'contact', 'name', 'mr', 'valery', 'grayfer', 'chairman', 'board', 'directors', 'pjsc', 'lukoil', '86', 'years', 'old', 'diagnosed', 'cancer', '2', 'years', 'ago', 'going', 'operation', 'later', 'week', 'decided', 'donate', 'sum', '8', '750', '000', '00', 'euros', 'eight', 'million', 'seven', 'hundred', 'fifty', 'thousand', 'euros', 'would', 'like', 'receive', 'donation', 'please', 'reply', 'contacting', 'operated']\n"
     ]
    }
   ],
   "source": [
    "# Check it\n",
    "print(no_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d97073",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Sentiment Analysis\n",
    "\n",
    "![](../images/sent.jpeg)\n",
    "\n",
    "[Sentiment analysis](https://www.kdnuggets.com/2018/08/emotion-sentiment-analysis-practitioners-guide-nlp-5.html) is an area of natural language processing in which we seek to classify text as having positive or negative emotion.\n",
    "\n",
    "Let's build a simple function that can classify text as either having positive or negative sentiment.\n",
    "\n",
    "What words tell us whether certain text is positive?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2731ded8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's come up with a list of positive and negative words we might observe. \n",
    "# Suggest more!!\n",
    "positive_words = ['love', 'good', 'great']\n",
    "negative_words = ['garbage', 'sad', 'bad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8164f939",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_sentiment(text):\n",
    "    # Instantiate tokenizer.\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')  \n",
    "    # Tokenize text.\n",
    "    tokens = tokenizer.tokenize(text.lower()) \n",
    "    # Instantiate stemmer.\n",
    "    p_stemmer = PorterStemmer()\n",
    "    # Stem words.\n",
    "    stemmed_words = [p_stemmer.stem(i) for i in tokens]\n",
    "    # Stem our positive/negative words.\n",
    "    positive_stems = [p_stemmer.stem(i) for i in positive_words]\n",
    "    negative_stems = [p_stemmer.stem(i) for i in negative_words]\n",
    "\n",
    "    # Count \"positive\" words.\n",
    "    positive_count = sum([1 for i in stemmed_words if i in positive_stems])\n",
    "    # Count \"negative\" words\n",
    "    negative_count = sum([1 for i in stemmed_words if i in negative_stems])\n",
    "    # Calculate Sentiment Percentage \n",
    "    return round((positive_count - negative_count) / len(tokens), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "86f1367e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run our sentiment analyzer \n",
    "simple_sentiment(\"I love programming!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a5b013",
   "metadata": {},
   "source": [
    "<details><summary> What are some shortcomings of this method? </summary>\n",
    "\n",
    "- Primarily, we're limited to the positive/negative words we came up with.\n",
    "- If someone wrote \"not good\" or \"not bad,\" our sentiment function would probably treat \"not good\" as positive or neutral... but it's probably supposed to mean negative!\n",
    "- The ordering of the words doesn't matter here, which is not how language generally works.\n",
    "- We haven't corrected for misspellings.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daac202a",
   "metadata": {},
   "source": [
    "There are a couple of ways to proceed with sentiment analysis:\n",
    "\n",
    "1. If you have already-labeled data, you can build a supervised learning model.\n",
    "2. If you don't have labeled data, you can use a Lexicon (dictionary) that has already been built/trained for sentiment analysis.\n",
    "    - There are a bunch of these and which to use depends on your purpose/data. Here are just a few that are available:\n",
    "        - AFINN lexicon\n",
    "        - MPQA subjectivity lexicon\n",
    "        - SentiWordNet\n",
    "        - VADER lexicon\n",
    "\n",
    "We will use the [VADER](https://www.nltk.org/_modules/nltk/sentiment/vader.html) (Valence Aware Dictionary and sEntiment Reasoner) lexicon to analyze the sentiments of our reviews.\n",
    "\n",
    "VADER has a basic lexicon for positive and negative sentiments, including emoticons and negation ('didn't like'), but also considers intensity in terms of use of CAPs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c32b1c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Sentiment Intensity Analyzer from nltk.sentiment.vader\n",
    "sent = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310689c2",
   "metadata": {},
   "source": [
    "VADER's `polarity_scores` takes in a string and returns a dictionary of scores in each of four categories:\n",
    "\n",
    "- negative\n",
    "- neutral\n",
    "- positive\n",
    "- compound, which calculates a score based on the above three. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "acf30931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.316, 'pos': 0.684, 'compound': 0.6523}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent.polarity_scores('This is FANTASTIC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e63e6992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.28, 'neu': 0.53, 'pos': 0.189, 'compound': -0.296}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent.polarity_scores('I don\"t like programming at all and I hate Python especially')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a8c1b4",
   "metadata": {},
   "source": [
    "Let's try analyzing the sentiment of IMDb movie reviews. The data is from [Kaggle](https://www.kaggle.com/c/word2vec-nlp-tutorial#part-1-for-beginners-bag-of-words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8d3553a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in training data.\n",
    "reviews = pd.read_csv(\"data/labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7be0a3dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"3630_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"It must be assumed that those who praised thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"9495_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review\n",
       "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
       "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
       "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n",
       "3  \"3630_4\"          0  \"It must be assumed that those who praised thi...\n",
       "4  \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ..."
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "871b67c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>\"3453_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"It seems like more consideration has gone int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>\"5064_1\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"I don't believe they made this film. Complete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>\"10905_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"Guy is a loser. Can't get girls, needs to bui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>\"10194_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"This 30 minute documentary Bu√±uel made in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>\"8478_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"I saw this movie as a child and it broke my h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id  sentiment                                             review\n",
       "24995   \"3453_3\"          0  \"It seems like more consideration has gone int...\n",
       "24996   \"5064_1\"          0  \"I don't believe they made this film. Complete...\n",
       "24997  \"10905_3\"          0  \"Guy is a loser. Can't get girls, needs to bui...\n",
       "24998  \"10194_3\"          0  \"This 30 minute documentary Bu√±uel made in the...\n",
       "24999   \"8478_8\"          1  \"I saw this movie as a child and it broke my h..."
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d553ab65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"There is a uk edition to this show which is rather less extravagant than the US version. The person concerned will get a new kitchen or perhaps bedroom and bathroom and is wonderfully grateful for what they have got. The US version of this show is everything that reality TV shouldn\\'t be. Instead of making a few improvements to a house which the occupants could not afford or do themselves the entire house gets rebuilt. I do not know if this show is trying to show what a lousy welfare system exists in the US or if you beg hard enough you will receive. The rather vulgar product placement that takes place, particularly by Sears, is also uncalled for. Rsther than turning one family in a deprived area into potential millionaires, it would be far better to help the community as a whole where instead of spending the hundreds of thousands of dollars on one home, build something for the whole community ..... perhaps a place where diy and power tools can be borrowed and returned along with building materials so that everyone can benefit should they want to. Giving it all to one person can cause enormous resentment among the rest of the local community who still live in the same run down houses.\"'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine a review.\n",
    "reviews['review'][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aae54706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.048, 'neu': 0.857, 'pos': 0.095, 'compound': 0.8625}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the sent.polarity_scores() to find the sentiment of the review.\n",
    "sent.polarity_scores(reviews['review'][100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "225aa914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Does this match the sentiment given in the training data?\n",
    "reviews['sentiment'][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6e1d2939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the scores to all the reviews\n",
    "reviews['scores'] = reviews['review'].apply(sent.polarity_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2f7706f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract only the compound score\n",
    "reviews['compound']  = reviews['scores'].apply(lambda score_dict: score_dict['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "87bee519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "      <th>scores</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "      <td>{'neg': 0.13, 'neu': 0.744, 'pos': 0.126, 'com...</td>\n",
       "      <td>-0.8278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "      <td>{'neg': 0.047, 'neu': 0.739, 'pos': 0.214, 'co...</td>\n",
       "      <td>0.9819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "      <td>{'neg': 0.142, 'neu': 0.8, 'pos': 0.058, 'comp...</td>\n",
       "      <td>-0.9883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"3630_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"It must be assumed that those who praised thi...</td>\n",
       "      <td>{'neg': 0.066, 'neu': 0.878, 'pos': 0.056, 'co...</td>\n",
       "      <td>-0.2189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"9495_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
       "      <td>{'neg': 0.119, 'neu': 0.741, 'pos': 0.14, 'com...</td>\n",
       "      <td>0.7960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"8196_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"I dont know why people think this is such a b...</td>\n",
       "      <td>{'neg': 0.183, 'neu': 0.594, 'pos': 0.223, 'co...</td>\n",
       "      <td>0.3935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\"7166_2\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"This movie could have been very good, but com...</td>\n",
       "      <td>{'neg': 0.163, 'neu': 0.709, 'pos': 0.129, 'co...</td>\n",
       "      <td>-0.6863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\"10633_1\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"I watched this video at a friend's house. I'm...</td>\n",
       "      <td>{'neg': 0.062, 'neu': 0.899, 'pos': 0.04, 'com...</td>\n",
       "      <td>-0.4517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\"319_1\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"A friend of mine bought this film for ¬£1, and...</td>\n",
       "      <td>{'neg': 0.072, 'neu': 0.736, 'pos': 0.192, 'co...</td>\n",
       "      <td>0.9707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\"8713_10\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"&lt;br /&gt;&lt;br /&gt;This movie is full of references....</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.805, 'pos': 0.195, 'comp...</td>\n",
       "      <td>0.8481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  sentiment                                             review  \\\n",
       "0   \"5814_8\"          1  \"With all this stuff going down at the moment ...   \n",
       "1   \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...   \n",
       "2   \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...   \n",
       "3   \"3630_4\"          0  \"It must be assumed that those who praised thi...   \n",
       "4   \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ...   \n",
       "5   \"8196_8\"          1  \"I dont know why people think this is such a b...   \n",
       "6   \"7166_2\"          0  \"This movie could have been very good, but com...   \n",
       "7  \"10633_1\"          0  \"I watched this video at a friend's house. I'm...   \n",
       "8    \"319_1\"          0  \"A friend of mine bought this film for ¬£1, and...   \n",
       "9  \"8713_10\"          1  \"<br /><br />This movie is full of references....   \n",
       "\n",
       "                                              scores  compound  \n",
       "0  {'neg': 0.13, 'neu': 0.744, 'pos': 0.126, 'com...   -0.8278  \n",
       "1  {'neg': 0.047, 'neu': 0.739, 'pos': 0.214, 'co...    0.9819  \n",
       "2  {'neg': 0.142, 'neu': 0.8, 'pos': 0.058, 'comp...   -0.9883  \n",
       "3  {'neg': 0.066, 'neu': 0.878, 'pos': 0.056, 'co...   -0.2189  \n",
       "4  {'neg': 0.119, 'neu': 0.741, 'pos': 0.14, 'com...    0.7960  \n",
       "5  {'neg': 0.183, 'neu': 0.594, 'pos': 0.223, 'co...    0.3935  \n",
       "6  {'neg': 0.163, 'neu': 0.709, 'pos': 0.129, 'co...   -0.6863  \n",
       "7  {'neg': 0.062, 'neu': 0.899, 'pos': 0.04, 'com...   -0.4517  \n",
       "8  {'neg': 0.072, 'neu': 0.736, 'pos': 0.192, 'co...    0.9707  \n",
       "9  {'neg': 0.0, 'neu': 0.805, 'pos': 0.195, 'comp...    0.8481  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for the first 10 reviews\n",
    "reviews.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2fe3c90b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"The film starts with a manager (Nicholas Bell) giving welcome investors (Robert Carradine) to Primal Park . A secret project mutating a primal animal using fossilized DNA, like ¬®Jurassik Park¬®, and some scientists resurrect one of nature\\'s most fearsome predators, the Sabretooth tiger or Smilodon . Scientific ambition turns deadly, however, and when the high voltage fence is opened the creature escape and begins savagely stalking its prey - the human visitors , tourists and scientific.Meanwhile some youngsters enter in the restricted area of the security center and are attacked by a pack of large pre-historical animals which are deadlier and bigger . In addition , a security agent (Stacy Haiduk) and her mate (Brian Wimmer) fight hardly against the carnivorous Smilodons. The Sabretooths, themselves , of course, are the real star stars and they are astounding terrifyingly though not convincing. The giant animals savagely are stalking its prey and the group run afoul and fight against one nature\\'s most fearsome predators. Furthermore a third Sabretooth more dangerous and slow stalks its victims.<br /><br />The movie delivers the goods with lots of blood and gore as beheading, hair-raising chills,full of scares when the Sabretooths appear with mediocre special effects.The story provides exciting and stirring entertainment but it results to be quite boring .The giant animals are majority made by computer generator and seem totally lousy .Middling performances though the players reacting appropriately to becoming food.Actors give vigorously physical performances dodging the beasts ,running,bound and leaps or dangling over walls . And it packs a ridiculous final deadly scene. No for small kids by realistic,gory and violent attack scenes . Other films about Sabretooths or Smilodon are the following : ¬®Sabretooth(2002)¬®by James R Hickox with Vanessa Angel, David Keith and John Rhys Davies and the much better ¬®10.000 BC(2006)¬® by Roland Emmerich with with Steven Strait, Cliff Curtis and Camilla Belle. This motion picture filled with bloody moments is badly directed by George Miller and with no originality because takes too many elements from previous films. Miller is an Australian director usually working for television (Tidal wave, Journey to the center of the earth, and many others) and occasionally for cinema ( The man from Snowy river, Zeus and Roxanne,Robinson Crusoe ). Rating : Below average, bottom of barrel.\"'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Have a look at one of the reviews that is different from the labelled sentiment\n",
    "reviews['review'][2]\n",
    "\n",
    "# What sentiment would YOU assign?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3307dfd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
