{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fd441c9",
   "metadata": {},
   "source": [
    "# Web Scraping Example\n",
    "\n",
    "\n",
    "## The Requests Library\n",
    "\n",
    "Now that we know how to use the Beautiful Soup library, let's perform the workflow for web scraping for data analysis.\n",
    "\n",
    "In this notebook, we will:\n",
    "1. Use the `Requests` library to get the HTML source for a web page\n",
    "1. Scrape the data using `Beautiful Soup`\n",
    "2. Save the results into a CSV file with `Pandas`.\n",
    "\n",
    "First we have to import the required libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d5f8d6",
   "metadata": {},
   "source": [
    "### A Note about web scraping\n",
    "\n",
    "Although you can basically scrape any website that you can read online, web scraping may be sending multiple requests to a website and this will increase the load on the web server. \n",
    "\n",
    "Some companies have terms and conditions regarding web scraping so you should always check. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a46c4a",
   "metadata": {},
   "source": [
    "### Scraping Sandbox\n",
    "\n",
    "We are going to use a site that is specifically developed to help us learn web scraping, called [toscrape.com](https://toscrape.com/). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4016cf9c",
   "metadata": {},
   "source": [
    "# Using the Requests Library\n",
    "\n",
    "We can use the `Requests` library to send a `get` request to a web page. If the response is of type HTML, then we can create a `BeautifulSoup` object by parsing the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a5fd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de86d9d7",
   "metadata": {},
   "source": [
    "Let's set up the URL then use the `Requests` library to get the web page which represents a [bookstore](http://books.toscrape.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bcbb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send a request to get the books.toscrape web page\n",
    "\n",
    "web_url = \"http://books.toscrape.com/\"\n",
    "response = requests.get(????)\n",
    "\n",
    "# parse the response using Beautiful Soup\n",
    "if (response.status_code == 200):\n",
    "    print(response.headers['Content-Type'])\n",
    "\n",
    "    print()\n",
    "else:\n",
    "    print(response.reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f697fdaa",
   "metadata": {},
   "source": [
    "# Using the Beautiful Soup Library\n",
    "\n",
    "If the content type of the response is \"text/html\", then we can proceed to create the `BeautifulSoup` object for scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9bd6f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.content,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f200d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84d77dd",
   "metadata": {},
   "source": [
    "Let's go through the list of categories. Inspecting the HTML through the `prettify()` output or *inspect element* in Google chrome, we want to find the tag `<div>` with the attribute `class=\"side_categories`, so let's try that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dece432e",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_list = soup.find(???, class_='???)\n",
    "category_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53604f41",
   "metadata": {},
   "source": [
    "Great! We have found it. Let's get all the `<a>`, or *anchor* tags which will lead us to each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b76976",
   "metadata": {},
   "outputs": [],
   "source": [
    "link_tags = category_list.find_all(???)\n",
    "links = [l.attrs for l in link_tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539d5243",
   "metadata": {},
   "outputs": [],
   "source": [
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05fe36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = web_url+links[1]['href']\n",
    "print(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ac4e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "link_request = requests.get(URL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe4aaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the response using Beautiful Soup\n",
    "if (link_request.status_code == 200):\n",
    "    print(link_request.headers['Content-Type'])\n",
    "\n",
    "    print()\n",
    "else:\n",
    "    print(link_request.reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f91bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_soup = BeautifulSoup(link_request.content,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f99ba49",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3df5721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can get the name of the category we are in from the <h1> tag.\n",
    "# Inspect the element to check!\n",
    "current_category = category_soup.h1.string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137469c5",
   "metadata": {},
   "source": [
    "Now that we are in the Travel books category, let's get the title, rating, price and status from each book.\n",
    "\n",
    "We can inspect the element to find the relevant tag.\n",
    "\n",
    "<img src=\"images/books_inspect.png\" alt=\"drawing\" width=\"200\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d74032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the first token in the class attribute to find the tag that contain the \n",
    "# information about books\n",
    "books = category_soup.find_all(???, class_=????)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2c844e",
   "metadata": {},
   "outputs": [],
   "source": [
    "books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25690393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's just get one book to inspect the tags closely\n",
    "one_book = books[0]\n",
    "one_book.contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5bbc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fdf51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the price\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd675486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the star rating\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1190463c",
   "metadata": {},
   "source": [
    "Now that we can get the data from one book, let's loop through all the books and store the title, price and star rating in respective lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca146ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = []\n",
    "titles = []\n",
    "prices = []\n",
    "ratings = []\n",
    "ratings_dict = {'One':1, 'Two':2, 'Three':3, 'Four':4, 'Five':5}\n",
    "for b in books:\n",
    "    ## Get the category\n",
    "    ## Get the title\n",
    "    ## get the price\n",
    "    ## get the rating\n",
    "    ## store the rating \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1181f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe from the lists\n",
    "books_df = pd.DataFrame({'Category':categories,\n",
    "                        'Title': titles,\n",
    "                        'Price':prices,\n",
    "                        'Rating':ratings})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06473a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f72f7b",
   "metadata": {},
   "source": [
    "**Saving to File**\n",
    "\n",
    "As you can see, we have a pretty nice data frame formed by pandas. It's not perfect, so we will have to do some data cleaning with additional pandas and Python functions.\n",
    "\n",
    "Now we can save this dataframe into a CSV file using the `to_csv()` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e1266e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe to file\n",
    "books_df.to_csv('scraped_books.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefab107",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "As you can see web scraping takes some exploration, inspecting and filtering of the various tags and attributes. In order to obtain all the books by category, we would have to loop through each of the links to send the request and then add the books from the returned responses, including navigating to additional pages. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e172f6d9",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "Let's try to extract some data from another wiki page, the [Quotes to Scrape](quotes.toscrape.com)\n",
    "\n",
    "Q1. Create a `BeautifulSoup` object to scrape the content from this page.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e935d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1 Answer\n",
    "\n",
    "#Send a request to get the HTML page http://quotes.toscrape.com\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3b33b6",
   "metadata": {},
   "source": [
    "Q2. Locate the tags which are in the \"Top Ten tags\" (view the page to check) and create a list with only these 10 tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742a4522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2 Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404e3305",
   "metadata": {},
   "source": [
    "Let's prompt the user which tag they would like to view quotes from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822a013c",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = input(\"Which types of quotes would you like to view?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7413d8db",
   "metadata": {},
   "source": [
    "Q3. Based on the user's answer, find the link to the page that contains the quotes for that tag. We can pass the string to search for as an argument in `find_all()` or `find()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf646e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put in the suitable tag and argument\n",
    "quote_tag = soup.find(???, string=???)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fcebe9",
   "metadata": {},
   "source": [
    "Q4. Send a request to get the quotes from that tag's page (remember to append the link to the correct web_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cf033d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4d02977",
   "metadata": {},
   "source": [
    "Q5. Store each of the quotes and their authors into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa2441a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd246862",
   "metadata": {},
   "source": [
    "Q6. Can you loop through each of the top ten tags to store all those quotes in a dataframe then save to a csv file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08820132",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
